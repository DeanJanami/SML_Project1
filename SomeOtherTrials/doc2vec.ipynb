{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display progress logs on stdout\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328932, 2)\n",
      "(35437, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "train_df = pd.read_csv('origin_data/train_tweets.txt', \n",
    "                 encoding=\"utf-8\",\n",
    "                 header=None, sep='\\t',\n",
    "                quoting=csv.QUOTE_NONE)\n",
    "# df = pd.DataFrame(data)\n",
    "train_df.columns = ['id', 'tweet']\n",
    "print(train_df.shape)\n",
    "\n",
    "unLabel_df = pd.read_csv('origin_data/test_tweets_unlabeled.txt', \n",
    "                      header=None,\n",
    "                      sep='\\t', \n",
    "                      quoting=csv.QUOTE_NONE)\n",
    "unLabel_df.columns = ['tweet']\n",
    "print(unLabel_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "To obtain the whole corpus, pre-processing both train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lower case\n",
    "2. Removing Punctuation\n",
    "3. Removal of Stop Words\n",
    "4. Common word removal\n",
    "5. Rare words removal\n",
    "6. Spelling correction\n",
    "7. Tokenization\n",
    "8. Lemmatization (not Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "def preProcess(originData):\n",
    "    res = copy.deepcopy(originData)\n",
    "    # Lower case\n",
    "    res['tweet'] = res['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    print(\"Lower case, Done\");\n",
    "    \n",
    "    # Removing Punctuation\n",
    "    res['tweet'] = res['tweet'].str.replace('[^\\w\\s]','')\n",
    "    print(\"Removing Punctuation, Done\");\n",
    "    \n",
    "    # Removal of Stop Words\n",
    "    res['tweet'] = res['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    print(\"Removal of Stop Words, Done\");\n",
    "    \n",
    "    # Common word removal\n",
    "    NUM_TOP_WORDS = 10\n",
    "    freq = pd.Series(' '.join(res['tweet']).split()).value_counts()[:NUM_TOP_WORDS]\n",
    "    freq_index = list(freq.index)\n",
    "    res['tweet'] = res['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    print(\"Common word removal, Done\");\n",
    "    \n",
    "    # Rare words removal\n",
    "    NUM_TAIL_WORDS = -10\n",
    "    freq = pd.Series(' '.join(res['tweet']).split()).value_counts()[NUM_TAIL_WORDS:]\n",
    "    freq_index = list(freq.index)\n",
    "    res['tweet'] = res['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    print(\"Rare word removal, Done\");\n",
    "    \n",
    "    # Spelling correction(to slow)\n",
    "#     res['tweet'] = res['tweet'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "#     print(\"Spelling correction, Done\");\n",
    "    \n",
    "    # Tokenization\n",
    "    res['tweet'] = res['tweet'].apply(lambda x: \" \".join(x for x in TextBlob(x).words))\n",
    "    print(\"Tokenization, Done\");\n",
    "    \n",
    "    # Lemmatization\n",
    "    res['tweet'] = res['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    print(\"Lemmatization, Done\");\n",
    "    return res\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower case, Done\n",
      "Removing Punctuation, Done\n",
      "Removal of Stop Words, Done\n",
      "Common word removal, Done\n",
      "Rare word removal, Done\n",
      "Tokenization, Done\n",
      "Lemmatization, Done\n"
     ]
    }
   ],
   "source": [
    "pre_train_df = preProcess(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328932, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower case, Done\n",
      "Removing Punctuation, Done\n",
      "Removal of Stop Words, Done\n",
      "Common word removal, Done\n",
      "Rare word removal, Done\n",
      "Tokenization, Done\n",
      "Lemmatization, Done\n"
     ]
    }
   ],
   "source": [
    "pre_unLabel_df = preProcess(unLabel_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35437, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_unLabel_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# path = os.getcwd() + '/data'\n",
    "# if not os.path.isdir(path):\n",
    "#     os.mkdir(path)\n",
    "    \n",
    "# # delete all\n",
    "# files = os.listdir(path)\n",
    "# for file in files:\n",
    "#     if os.path.exists(file):\n",
    "#         os.remove(file)\n",
    "\n",
    "# for i in range(pre_train_df.id.size):\n",
    "#     with open(path + '/' + str(pre_train_df.id[i]) + '.txt', mode='a+') as f:\n",
    "#         f.write(pre_train_df.tweet[i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort for train sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>328529</th>\n",
       "      <td>2</td>\n",
       "      <td>mike look little bit harrey carrey glass httpt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328537</th>\n",
       "      <td>2</td>\n",
       "      <td>actually almost entire elton john catalogdigit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328536</th>\n",
       "      <td>2</td>\n",
       "      <td>really want elton john album cant justify purc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328535</th>\n",
       "      <td>2</td>\n",
       "      <td>cube buddy elton john morningnothing music kee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328534</th>\n",
       "      <td>2</td>\n",
       "      <td>announcing winner noon giving 10 follower craf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet\n",
       "328529   2  mike look little bit harrey carrey glass httpt...\n",
       "328537   2  actually almost entire elton john catalogdigit...\n",
       "328536   2  really want elton john album cant justify purc...\n",
       "328535   2  cube buddy elton john morningnothing music kee...\n",
       "328534   2  announcing winner noon giving 10 follower craf..."
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_pre_train_df = pre_train_df.sort_values(by = \"id\")\n",
    "sort_pre_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a corpus as training set for doc2vec\n",
    "To train doc2vec model, we need to create a training set consist of line of tweets. One tweet per line, one line per tweet. \n",
    "\n",
    "Further research is needed to find out whether preprocessing on the tweets here are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"ebd_train.txt\", \"w\") as f:\n",
    "#     for tweet in train_df.tweet:\n",
    "#         f.write(str(tweet) + '\\n')\n",
    "#     for tweet in unLabel_df.tweet:\n",
    "#         f.write(str(tweet) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the doc2vec model with the embedding training set and save the model. \n",
    "\n",
    "Since it takes a long time to train, it is better to load the model I have trained called **model.bin**.\n",
    "\n",
    "If you want to train it yourself, please run it in terminal using `python3 train_doc2vec_model.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## infer vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-08 15:50:45,076 loading Doc2Vec object from model300-100.bin\n",
      "/Users/Aaron-Qiu/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-09-08 15:50:45,620 loading vocabulary recursively from model300-100.bin.vocabulary.* with mmap=None\n",
      "2019-09-08 15:50:45,621 loading trainables recursively from model300-100.bin.trainables.* with mmap=None\n",
      "2019-09-08 15:50:45,621 loading syn1neg from model300-100.bin.trainables.syn1neg.npy with mmap=None\n",
      "2019-09-08 15:50:45,636 loading wv recursively from model300-100.bin.wv.* with mmap=None\n",
      "2019-09-08 15:50:45,637 loading vectors from model300-100.bin.wv.vectors.npy with mmap=None\n",
      "2019-09-08 15:50:45,673 loading docvecs recursively from model300-100.bin.docvecs.* with mmap=None\n",
      "2019-09-08 15:50:45,674 loading vectors_docs from model300-100.bin.docvecs.vectors_docs.npy with mmap=None\n",
      "2019-09-08 15:50:45,885 loaded model300-100.bin\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "modelPath = \"model300-100.bin\"\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "model = Doc2Vec.load(modelPath)\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference hyper-parameters\n",
    "# start_alpha=0.01\n",
    "# infer_epoch=100\n",
    "\n",
    "train_vecs = []\n",
    "# for x in pre_train_df.tweet:\n",
    "#     train_vecs.append(model.infer_vector(x.split(), alpha=start_alpha, steps=infer_epoch))\n",
    "for x in sort_pre_train_df.tweet:\n",
    "    train_vecs.append(model.infer_vector(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "unLabel_vecs = []\n",
    "for x in sort_pre_train_df.tweet:\n",
    "    unLabel_vecs.append(model.infer_vector(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to file, no need to train every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try PCA\n",
    "Not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.99, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(0.99)\n",
    "pca.fit(train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Aaron-Qiu/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.externals import joblib\n",
    "model_save_path = \"model_save/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "\n",
    "Training: O(m^2N^2)，Predicting: O(m^2N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_vecs[:2000], \n",
    "                                                    sort_pre_train_df.id[:2000], \n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=24) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "x_train_pca = pca.transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set...\n",
      "done in 2.077s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train a SVM classification model\n",
    "print(\"Fitting the classifier to the training set...\")\n",
    "t0 = time()\n",
    "# 0 < C <= 1. Larger better, but may cause overfit\n",
    "# kernel='rbf'时（default），为高斯核，\n",
    "# gamma值越小，分类界面越连续；\n",
    "# gamma值越大，分类界面越“散”，分类效果越好，但有可能会过拟合。\n",
    "svm_clf = SVC(kernel='rbf', gamma=1000)\n",
    "\n",
    "svm_clf.fit(x_train_pca, y_train)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "done in 1.183s\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting...\")\n",
    "t0 = time()\n",
    "predict_train = svm_clf.predict(x_train_pca)\n",
    "predict_test = svm_clf.predict(x_test_pca)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 for train =  0.965\n",
      "f1 for test =  0.0675\n"
     ]
    }
   ],
   "source": [
    "print('f1 for train = ' , f1_score(y_train, predict_train, average='micro'))\n",
    "print('f1 for test = ' , f1_score(y_test, predict_test, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_save/svm_train_model.m']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path_name = model_save_path + \"svm_\" + \"train_model.m\"\n",
    "joblib.dump(svm_clf, save_path_name)\n",
    "# clf = joblib.load(save_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set...\n",
      "done in 2.498s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn import tree\n",
    "print(\"Fitting the classifier to the training set...\")\n",
    "t0 = time()\n",
    "dt_clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "dt_clf.fit(x_train_pca, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "done in 0.004s\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting...\")\n",
    "t0 = time()\n",
    "predict_train = dt_clf.predict(x_train_pca)\n",
    "predict_test = dt_clf.predict(x_test_pca)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 for train =  0.998125\n",
      "f1 for test =  0.0625\n"
     ]
    }
   ],
   "source": [
    "print('f1 for train = ' , f1_score(y_train, predict_train, average='micro'))\n",
    "print('f1 for test = ' , f1_score(y_test, predict_test, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cnn classifier based on docvecs.\n",
    "1. First, read (docvec,author) pairs from training and test set.\n",
    "2. Second, build a CNN with keras.\n",
    "3. Then, train the CNN with training set.\n",
    "4. Finally, test the CNN with test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'watch', 'grey', 'big', 'screen', 'thursday', 'indulgence']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_df.tweet[1].split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submision Creating\n",
    "Predict Submission:\n",
    "https://www.kaggle.com/t/cb6ceb3bf96a48819d6b4f0994fb58db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_answer = svm_clf.predict(pca.transform(unLabel_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35437"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission.txt\", \"w\") as f:\n",
    "    f.write('Id,Predicted\\n')\n",
    "    index = 0\n",
    "    for i in predict_answer:\n",
    "        index += 1\n",
    "        f.write(str(index) + ',' + str(i) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
