{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temp text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import libraries successful!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "# Above is directly taken from worksheet 3\n",
    "print('Import libraries successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data the standard way - better to use panda?\n",
    "# trainFile = open('train_tweets.txt',  encoding=\"utf8\")\n",
    "# train = trainFile.read()\n",
    "\n",
    "# testFile = open('test_tweets_unlabeled.txt',  encoding=\"utf8\")\n",
    "# test = testFile.read()\n",
    "# print('Import files successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Attempt to vectorise - failed\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# vec = DictVectorizer()\n",
    "# vec.fit_transform(train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import and shift successful in panda!\n",
      "Adding length of tweets!\n",
      "Adding the number of charecters in a tweet\n"
     ]
    }
   ],
   "source": [
    "# Import file using panda - seperated by tab and transforfmed to CSV to allow for features\n",
    "# The .head() just prints off the first few lines of the array, except it seems to be overridden \n",
    "# by the print statement\n",
    "data = pd.read_csv('train_tweets.txt', sep=\"\\t\", header=None)\n",
    "data.columns = ['id','tweet']\n",
    "print('Import and shift successful in panda!')\n",
    "\n",
    "#Number of words\n",
    "data['word_count'] = data['tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "# data[['tweet','word_count']].head()\n",
    "print('Adding length of tweets!')\n",
    "\n",
    "# Number of charecters\n",
    "data['char_count'] = data['tweet'].str.len() ## this also includes spaces\n",
    "data[['tweet','char_count']].head()\n",
    "print('Adding the number of charecters in a tweet')\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word length\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "data['avg_word'] = data['tweet'].apply(lambda x: avg_word(x))\n",
    "data[['tweet','avg_word']].head()\n",
    "print('Added average words!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - run this if the first time in anaconda\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# Removing stopwords (the,a,we,can,..)\n",
    "# the stopwords are quite aggressive so might omit or will create a seperate version\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "stop = stopwords.words('english')\n",
    "# stop = set(stopwords.words('english'))\n",
    "# Get the number of stop words\n",
    "data['stopwords'] = data['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "data[['tweet','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import TweetTokenizer\n",
    "# tknzr = TweetTokenizer()\n",
    "# s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "# tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of special charecters\n",
    "data['hastags'] = data['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "data[['tweet','hastags']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of numerics\n",
    "data['numerics'] = data['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "data[['tweet','numerics']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPPERCASE words\n",
    "data['upper'] = data['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "data[['tweet','upper']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       @handle let's try and catch up live next week!\n",
       "1    going to watch grey's on the big screen - thur...\n",
       "2    @handle my pleasure patrick....hope you are well!\n",
       "3    @handle hi there! been traveling a lot and lot...\n",
       "4    rt @handle looking to drink clean & go green? ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts all tweets into lowercase to avoid multiple copies of words\n",
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          handle lets try and catch up live next week\n",
       "1    going to watch greys on the big screen  thursd...\n",
       "2          handle my pleasure patrickhope you are well\n",
       "3    handle hi there been traveling a lot and lots ...\n",
       "4    rt handle looking to drink clean  go green pur...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes all punctuation - reduces training data\n",
    "data['tweet'] = data['tweet'].str.replace('[^\\w\\s]','')\n",
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 handle lets try catch live next week\n",
       "1     going watch greys big screen thursday indulgence\n",
       "2                     handle pleasure patrickhope well\n",
       "3    handle hi traveling lot lots come next month r...\n",
       "4    rt handle looking drink clean go green purchas...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        lets try catch live next week\n",
       "1     going watch greys big screen thursday indulgence\n",
       "2                            pleasure patrickhope well\n",
       "3    hi traveling lot lots come next month recovere...\n",
       "4    looking drink clean go green purchase clear2go...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the 10 most common words - do before or after stop words? (It removes handle...)\n",
    "freq = pd.Series(' '.join(data['tweet']).split()).value_counts()[:10]\n",
    "freq = list(freq.index)\n",
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 least common words - worth removing??\n",
    "inFreq = pd.Series(' '.join(data['tweet']).split()).value_counts()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        lets try catch live next week\n",
       "1      going watch grey big screen thursday indulgence\n",
       "2                            pleasure patrickhope well\n",
       "3    hi traveling lot lots come next month recovere...\n",
       "4    looking drink clean go green purchase clear2go...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spelling correction - works now\n",
    "# Need to install textblob via conda -> run the below statement in commandprompt/terminal\n",
    "# conda install -c conda-forge textblob\n",
    "from textblob import TextBlob\n",
    "data['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         let try catch live next week\n",
       "1      going watch grey big screen thursday indulgence\n",
       "2                            pleasure patrickhope well\n",
       "3    hi traveling lot lot come next month recovered...\n",
       "4    looking drink clean go green purchase clear2go...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmenisation - works now\n",
    "# run below download if it is the first time running\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from textblob import Word\n",
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(data['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 295375 instances. Test set has 32820 instances.\n"
     ]
    }
   ],
   "source": [
    "# Split our LABELED training data (80/20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_bow = bow[:328195,:]\n",
    "test_bow = bow[328195:,:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_bow, data['id'], test_size=0.10, random_state=42)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))\n",
    "# features = ['stopwords']\n",
    "# x = X_train[features].values.ravel()\n",
    "# x_test = X_test[features].values.ravel()\n",
    "# y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "lreg = LogisticRegression(tol=0.1)\n",
    "# lreg.fit(X_train, y_train)\n",
    "\n",
    "# prediction = lreg.predict_proba(X_test) # predicting on the validation set\n",
    "# prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "# prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "# f1_score(y_test, prediction_int) # calculating f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 487)\t0.4029702449636645\n",
      "  (0, 888)\t0.4595095707564396\n",
      "  (0, 136)\t0.5491106012964109\n",
      "  (0, 499)\t0.41379132449598516\n",
      "  (0, 944)\t0.39207170692732896\n",
      "  (1, 358)\t0.4192822537381518\n",
      "  (1, 930)\t0.48740297801705434\n",
      "  (1, 88)\t0.4836873179909417\n",
      "  (1, 854)\t0.5938748244664801\n",
      "  (3, 407)\t0.4289352032343252\n",
      "  (3, 511)\t0.7431450449048352\n",
      "  (3, 171)\t0.3261312140797652\n",
      "  (3, 562)\t0.39672215070824707\n",
      "  (4, 506)\t0.42752015857187725\n",
      "  (4, 247)\t0.5118130229162409\n",
      "  (4, 159)\t0.5633274431421991\n",
      "  (4, 369)\t0.4877869774716098\n",
      "  (5, 336)\t0.5893639940772505\n",
      "  (5, 417)\t0.591553840042965\n",
      "  (5, 596)\t0.5501946353934513\n",
      "  (6, 346)\t0.7557917545557259\n",
      "  (6, 525)\t0.6548120522299337\n",
      "  (7, 447)\t1.0\n",
      "  (8, 850)\t0.5645751428517962\n",
      "  (8, 335)\t0.6312517170075768\n",
      "  :\t:\n",
      "  (328185, 687)\t0.42396368258468153\n",
      "  (328185, 812)\t0.46966600498554084\n",
      "  (328186, 914)\t1.0\n",
      "  (328187, 556)\t0.4447918911303758\n",
      "  (328187, 838)\t0.8956339506654848\n",
      "  (328188, 973)\t0.6281184846323796\n",
      "  (328188, 687)\t0.7781177091309021\n",
      "  (328189, 287)\t0.7434192701756377\n",
      "  (328189, 837)\t0.6688256788816664\n",
      "  (328190, 444)\t0.5690006585609191\n",
      "  (328190, 914)\t0.3741655989845484\n",
      "  (328190, 779)\t0.5675102969208379\n",
      "  (328190, 603)\t0.4627854988897089\n",
      "  (328191, 403)\t0.4313404104155001\n",
      "  (328191, 119)\t0.4433927931255148\n",
      "  (328191, 771)\t0.5350171631363214\n",
      "  (328191, 701)\t0.5754171673633878\n",
      "  (328192, 576)\t0.7962598207523623\n",
      "  (328192, 914)\t0.6049547899268309\n",
      "  (328193, 525)\t0.4235594776692946\n",
      "  (328193, 63)\t0.5992726397363223\n",
      "  (328193, 779)\t0.6793155909737202\n",
      "  (328194, 436)\t0.5593964303377708\n",
      "  (328194, 733)\t0.5904063020555276\n",
      "  (328194, 779)\t0.581804118426878\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(data['tweet'])\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_tfidf = tfidf[:328195,:]\n",
    "test_tfidf = tfidf[328195:,:]\n",
    "\n",
    "xtrain_tfidf = train_tfidf[y_train.index]\n",
    "xvalid_tfidf = train_tfidf[y_test.index]\n",
    "\n",
    "# lreg = LogisticRegression(tol=0.1)\n",
    "# lreg.fit(xtrain_tfidf, y_train)\n",
    "\n",
    "# prediction = lreg.predict_proba(xvalid_tfidf)\n",
    "# prediction_int = prediction[:,1] >= 0.3\n",
    "# prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "# f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# X, y = load_iris(return_X_y=True)\n",
    "# clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "#                           multi_class='multinomial').fit(X, y)\n",
    "# print(len(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
