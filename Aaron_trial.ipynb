{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "## Predict Submission\n",
    "https://www.kaggle.com/t/cb6ceb3bf96a48819d6b4f0994fb58db\n",
    "## Features\n",
    "### Good features for this problem\n",
    "1. are able capture the distinctive aspects of someone’s writing style, and \n",
    "2. are consistent even when the author is writing on different subjects.\n",
    "\n",
    "### Features may works\n",
    "- Lexical features:\n",
    " - The average number of words per sentence\n",
    " - Sentence length variation\n",
    " - Lexical diversity, which is a measure of the richness of the author’s vocabulary\n",
    "- Punctuation features:\n",
    " - Average number of commas, semicolons and colons per sentence\n",
    "- average length of words\n",
    "- the frequency of digits used\n",
    "- the frequency of letters used\n",
    "\n",
    "## References\n",
    "Authorship Attribution with Python\n",
    "http://www.aicbt.com/authorship-attribution/\n",
    "\n",
    "Ultimate guide to deal with Text Data\n",
    "https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "\n",
    "Comprehensive Hands on Guide to Twitter Sentiment Analysis with dataset and code\n",
    "https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/\n",
    "\n",
    "## Supervised Learning\n",
    "(need labels, gather ground truth from external source)\n",
    "k-Nearest Neighbors\n",
    "Linear Regression\n",
    "Logistic Regression\n",
    "Support Vector Machines（SVMs）\n",
    "Decision Trees and Random Forests\n",
    "Neural networks\n",
    "\n",
    "## Unsupervised Learning\n",
    "(do not need labels, the analysis is conducted without ground truth. )\n",
    "- Clustering\n",
    " - k-Means\n",
    " - Hierarchical Cluster Analysis（HCA）\n",
    " - Expectation Maximization\n",
    " \n",
    "- Visualization and dimensionality reduction\n",
    " - Principal Component Analysis（PCA）\n",
    " - Kernel PCA\n",
    " - Locally-Linear Embedding（LLE）\n",
    " - t-distributed Stochastic Neighbor Embedding（t-SNE）\n",
    "- Association rule learning\n",
    " - Apriori\n",
    " - Ecla\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Trials\n",
    "consider the unsupervised problem. There are three steps:\n",
    "\n",
    "1. Preparing and loading the data\n",
    "2. Feature extraction: We will experiment with a few different feature sets. Even though the focus is on the unsupervised problem, the feature extraction code can also be used for supervised learning.\n",
    "3. Classification: We will use clustering to find natural groupings in the data. Since we have several feature sets, we will use ensemble learning: learn multiple models, each built using different features, that vote to determine who wrote each chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Create Global Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn\n",
    "import nltk\n",
    "import copy\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer # removal of suffices, like “ing”, “ly”, “s”, etc.\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "def readLabelData(path):\n",
    "    fo = open(path, \"r\")\n",
    "    data = fo.readlines();\n",
    "    fo.close()\n",
    "    res = []\n",
    "    for x in data:\n",
    "        x = x.rstrip('\\n')\n",
    "        id, tweet = x.split('\\t')\n",
    "        res.append([int(id), tweet])\n",
    "    return res\n",
    "\n",
    "def readUnlabelData(path):\n",
    "    fo = open(path, \"r\")\n",
    "    data = fo.readlines();\n",
    "    fo.close()\n",
    "    res = []\n",
    "    for x in data:\n",
    "        x = x.rstrip('\\n')\n",
    "        res.append(x)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328932, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/train_tweets.txt', \n",
    "                 encoding=\"utf-8\",\n",
    "                 header=None, sep='\\t',\n",
    "                quoting=csv.QUOTE_NONE)\n",
    "# data = readLabelData('data/train_tweets.txt')\n",
    "# df = pd.DataFrame(data)\n",
    "df.columns = ['id', 'tweet']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of words\n",
    "2. Number of characters(with spaces or without space is giving)\n",
    "3. Average Word Length\n",
    "4. Number of stopwords\n",
    "5. Number of special characters\n",
    "6. Number of numerics\n",
    "7. Number of Uppercase words\n",
    "8. Number of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeature(df):\n",
    "    df['words'] = df['tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df['chars'] = df['tweet'].str.len() ## this also includes spaces\n",
    "# charNum = df['tweet'].apply(lambda x: \n",
    "#                                    len(str(x).replace(\" \", \"\")))\n",
    "    df['avg_word'] = df['tweet'].apply(lambda x: avg_word(x))\n",
    "    df['stopwords'] = df['tweet'].apply(lambda x: \n",
    "                                    len([x for x in x.split() \n",
    "                                         if x in stop]))\n",
    "    df['hastags'] = df['tweet'].apply(lambda x: \n",
    "                               len([x for x in x.split() \n",
    "                                    if x.startswith('#')]))\n",
    "    df['numerics'] = df['tweet'].apply(lambda x: \n",
    "                             len([x for x in x.split() \n",
    "                                  if x.isdigit()]))\n",
    "    df['upper'] = df['tweet'].apply(lambda x: \n",
    "                          len([x for x in x.split() \n",
    "                               if x.isupper()]))\n",
    "    df['punctuation'] = df['tweet'].str.replace('[\\w\\s]','').apply(lambda x:\n",
    "                                                        len(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words'] = df['tweet'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['chars'] = df['tweet'].str.len() ## this also includes spaces\n",
    "# charNum = df['tweet'].apply(lambda x: \n",
    "#                                    len(str(x).replace(\" \", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_word'] = df['tweet'].apply(lambda x: avg_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stopwords'] = df['tweet'].apply(lambda x: \n",
    "                                    len([x for x in x.split() \n",
    "                                         if x in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hastags'] = df['tweet'].apply(lambda x: \n",
    "                               len([x for x in x.split() \n",
    "                                    if x.startswith('#')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['numerics'] = df['tweet'].apply(lambda x: \n",
    "                             len([x for x in x.split() \n",
    "                                  if x.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['upper'] = df['tweet'].apply(lambda x: \n",
    "                          len([x for x in x.split() \n",
    "                               if x.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['punctuation'] = df['tweet'].str.replace('[\\w\\s]','').apply(lambda x:\n",
    "                                                        len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hastags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8746</td>\n",
       "      <td>@handle Let's try and catch up live next week!</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>4.222222</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8746</td>\n",
       "      <td>Going to watch Grey's on the big screen - Thur...</td>\n",
       "      <td>11</td>\n",
       "      <td>66</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8746</td>\n",
       "      <td>@handle My pleasure Patrick....hope you are well!</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>6.142857</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8746</td>\n",
       "      <td>@handle Hi there! Been traveling a lot and lot...</td>\n",
       "      <td>27</td>\n",
       "      <td>132</td>\n",
       "      <td>3.925926</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8746</td>\n",
       "      <td>RT @handle Looking to Drink Clean &amp; Go Green? ...</td>\n",
       "      <td>19</td>\n",
       "      <td>109</td>\n",
       "      <td>4.789474</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              tweet  words  chars  \\\n",
       "0  8746     @handle Let's try and catch up live next week!      9     46   \n",
       "1  8746  Going to watch Grey's on the big screen - Thur...     11     66   \n",
       "2  8746  @handle My pleasure Patrick....hope you are well!      7     49   \n",
       "3  8746  @handle Hi there! Been traveling a lot and lot...     27    132   \n",
       "4  8746  RT @handle Looking to Drink Clean & Go Green? ...     19    109   \n",
       "\n",
       "   avg_word  stopwords  hastags  numerics  upper  punctuation  \n",
       "0  4.222222          2        0         0      0            3  \n",
       "1  5.090909          3        0         0      0            7  \n",
       "2  6.142857          2        0         0      0            6  \n",
       "3  3.925926          9        0         0      0            6  \n",
       "4  4.789474          4        0         0      1            5  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pre-processing (Don't Run)\n",
    "cleaning the data in order to obtain better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preProcess = copy.deepcopy(df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Lower case\n",
    "preProcess = preProcess.apply(lambda x: \n",
    "                                      \" \".join(x.lower() \n",
    "                                               for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Removing Punctuation\n",
    "preProcess = preProcess.str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Removal of Stop Words\n",
    "preProcess = preProcess.apply(lambda x: \n",
    "                                \" \".join(x for x in x.split() \n",
    "                                         if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Common word removal\n",
    "NUM_TOP_WORDS = 10\n",
    "freq = pd.Series(' '.join(preProcess).split()).value_counts()[:NUM_TOP_WORDS]\n",
    "freq_index = list(freq.index)\n",
    "preProcess = preProcess.apply(lambda x: \n",
    "                                      \" \".join(x for x in x.split() \n",
    "                                               if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Rare words removal\n",
    "NUM_TAIL_WORDS = -10\n",
    "freq = pd.Series(' '.join(preProcess).split()).value_counts()[NUM_TAIL_WORDS:]\n",
    "freq_index = list(freq.index)\n",
    "preProcess = preProcess.apply(lambda x: \n",
    "                                \" \".join(x for x in x.split() \n",
    "                                         if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Spelling correction(take a lot of time)\n",
    "# preProcess.apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Tokenization\n",
    "# TextBlob(preProcess[1]).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Stemming\n",
    "# st = PorterStemmer()\n",
    "# preProcess.apply(lambda x: \n",
    "#                  \" \".join([st.stem(word) \n",
    "#                            for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Lemmatization\n",
    "# usually prefer using lemmatization over stemming.\n",
    "preProcess = preProcess.apply(lambda x: \n",
    "                              \" \".join([Word(word).lemmatize() \n",
    "                                        for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hastags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8746</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>4.222222</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8746</td>\n",
       "      <td>11</td>\n",
       "      <td>66</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8746</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>6.142857</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8746</td>\n",
       "      <td>27</td>\n",
       "      <td>132</td>\n",
       "      <td>3.925926</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8746</td>\n",
       "      <td>19</td>\n",
       "      <td>109</td>\n",
       "      <td>4.789474</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  words  chars  avg_word  stopwords  hastags  numerics  upper  \\\n",
       "0  8746      9     46  4.222222          2        0         0      0   \n",
       "1  8746     11     66  5.090909          3        0         0      0   \n",
       "2  8746      7     49  6.142857          2        0         0      0   \n",
       "3  8746     27    132  3.925926          9        0         0      0   \n",
       "4  8746     19    109  4.789474          4        0         0      1   \n",
       "\n",
       "   punctuation  \n",
       "0            3  \n",
       "1            7  \n",
       "2            6  \n",
       "3            6  \n",
       "4            5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['words','chars','avg_word','stopwords','hastags','numerics','upper','punctuation']\n",
    "df[['id'] + features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hastags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>328932.000000</td>\n",
       "      <td>328932.000000</td>\n",
       "      <td>328932.000000</td>\n",
       "      <td>328932.000000</td>\n",
       "      <td>328932.000000</td>\n",
       "      <td>328932.000000</td>\n",
       "      <td>328932.000000</td>\n",
       "      <td>328932.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.708602</td>\n",
       "      <td>84.229306</td>\n",
       "      <td>5.600385</td>\n",
       "      <td>3.657908</td>\n",
       "      <td>0.154175</td>\n",
       "      <td>0.119003</td>\n",
       "      <td>0.771260</td>\n",
       "      <td>6.354721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.657503</td>\n",
       "      <td>37.117228</td>\n",
       "      <td>2.445895</td>\n",
       "      <td>3.118349</td>\n",
       "      <td>0.556568</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>1.538788</td>\n",
       "      <td>4.185560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>5.142857</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               words          chars       avg_word      stopwords  \\\n",
       "count  328932.000000  328932.000000  328932.000000  328932.000000   \n",
       "mean       13.708602      84.229306       5.600385       3.657908   \n",
       "std         6.657503      37.117228       2.445895       3.118349   \n",
       "min         1.000000       1.000000       1.000000       0.000000   \n",
       "25%         8.000000      55.000000       4.400000       1.000000   \n",
       "50%        13.000000      85.000000       5.142857       3.000000   \n",
       "75%        19.000000     118.000000       6.250000       6.000000   \n",
       "max        38.000000     150.000000     140.000000      24.000000   \n",
       "\n",
       "             hastags       numerics          upper    punctuation  \n",
       "count  328932.000000  328932.000000  328932.000000  328932.000000  \n",
       "mean        0.154175       0.119003       0.771260       6.354721  \n",
       "std         0.556568       0.396859       1.538788       4.185560  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000       3.000000  \n",
       "50%         0.000000       0.000000       0.000000       6.000000  \n",
       "75%         0.000000       0.000000       1.000000       9.000000  \n",
       "max        17.000000      14.000000      31.000000     128.000000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[features], \n",
    "                                                    df.id, \n",
    "                                                    test_size=0.1,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05175706, 0.24713924, 0.25608045, 0.15293236, 0.02665349,\n",
       "       0.01810787, 0.07846067, 0.16886885])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(x_train, y_train)\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 for train =  0.5889277727859261\n",
      "f1 for test =  0.04651304189213838\n"
     ]
    }
   ],
   "source": [
    "predict_train = clf.predict(x_train)\n",
    "predict_test = clf.predict(x_test)\n",
    "print('f1 for train = ' , f1_score(y_train, predict_train, average='micro'))\n",
    "print('f1 for test = ' , f1_score(y_test, predict_test, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the whole data\n",
    "# clf.fit(df[features], df.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Naive Bayes (work like shit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 for train =  0.0003276606381613171\n",
      "f1 for test =  0.00024320544780203075\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf_bayes = linear_model.BayesianRidge()\n",
    "clf_bayes.fit(x_train, y_train)\n",
    "predict_train = clf_bayes.predict(x_train).round() #取整\n",
    " = clf_bayes.predict(x_test).round()\n",
    "\n",
    "print('f1 for train = ' , f1_score(y_train, predict_train, average='micro'))\n",
    "print('f1 for test = ' , f1_score(y_test, predict_test, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Kmeans (more worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 2 ... 3 0 1]\n",
      "f1 for train =  2.3645613063187833e-05\n",
      "f1 for test =  0.00012160272390101538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster   import KMeans \n",
    "#使用默认的K-Means算法  \n",
    "num_clusters = 4\n",
    "kmeans_clf = KMeans(n_clusters=num_clusters)  \n",
    "kmeans_clf.fit(x_train)  \n",
    "\n",
    "predict_train = kmeans_clf.predict(x_train)\n",
    "predict_test = kmeans_clf.predict(x_test)\n",
    "print('f1 for train = ' , f1_score(y_train, predict_train, average='micro'))\n",
    "print('f1 for test = ' , f1_score(y_test, predict_test, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 for train =  0.20923665205142583\n",
      "f1 for test =  0.035477594698121236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(x_train,y_train)\n",
    "predict_train = knn_clf.predict(x_train)\n",
    "predict_test = knn_clf.predict(x_test)\n",
    "print('f1 for train = ' , f1_score(y_train, predict_train, average='micro'))\n",
    "print('f1 for test = ' , f1_score(y_test, predict_test, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submision Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35437, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unLabel = pd.read_csv('data/test_tweets_unlabeled.txt', \n",
    "                      header=None,\n",
    "                      sep='\\t', \n",
    "                      quoting=csv.QUOTE_NONE)\n",
    "# data = readUnlabelData('data/test_tweets_unlabeled.txt')\n",
    "# unLabel = pd.DataFrame(data)\n",
    "unLabel.columns = ['tweet']\n",
    "unLabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "createFeature(unLabel)\n",
    "predict_answer = clf.predict(unLabel[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hastags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Some people say that rappers don’t have feelin...</td>\n",
       "      <td>23</td>\n",
       "      <td>133</td>\n",
       "      <td>4.826087</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you know how to tweet on a Blackberry 8830?...</td>\n",
       "      <td>15</td>\n",
       "      <td>92</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Yoga is the cessation of mind.\" -Patanjali</td>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "      <td>5.285714</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@handle Well, with my millions of dollars, a f...</td>\n",
       "      <td>18</td>\n",
       "      <td>99</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cambria hotels free guide http://hotels.izigot...</td>\n",
       "      <td>13</td>\n",
       "      <td>118</td>\n",
       "      <td>8.153846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  words  chars  avg_word  \\\n",
       "0  Some people say that rappers don’t have feelin...     23    133  4.826087   \n",
       "1  Do you know how to tweet on a Blackberry 8830?...     15     92  5.200000   \n",
       "2        \"Yoga is the cessation of mind.\" -Patanjali      7     43  5.285714   \n",
       "3  @handle Well, with my millions of dollars, a f...     18     99  4.555556   \n",
       "4  Cambria hotels free guide http://hotels.izigot...     13    118  8.153846   \n",
       "\n",
       "   stopwords  hastags  numerics  upper  punctuation  \n",
       "0          8        0         0      0            5  \n",
       "1          6        0         0      0            3  \n",
       "2          3        0         0      0            4  \n",
       "3          9        0         0      0            7  \n",
       "4          0        0         1      0            9  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unLabel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission.txt\", \"w\") as f:\n",
    "    f.write('Id,Predicted\\n')\n",
    "    index = 0\n",
    "    for i in predict_answer:\n",
    "        index += 1\n",
    "        f.write(str(index) + ',' + str(i) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计词频\n",
    "nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "matchObj = re.match(r'\\S'*)\n",
    "s1 = \"asad a a sdas da as das \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(preProcess[0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency\n",
    "TF = (Number of times term T appears in the particular row) / (number of terms in that row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = (preProcess[0:5]).apply(lambda x: \n",
    "                              pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency\n",
    "IDF = log(N/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "  tf1.loc[i, 'idf'] = np.log(preProcess.shape[0]/(len(preProcess[preProcess.str.contains(word)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn has a separate function to directly obtain it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(preProcess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(preProcess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
