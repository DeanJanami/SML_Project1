{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "## Features\n",
    "### Good features for this problem\n",
    "1. are able capture the distinctive aspects of someone’s writing style, and \n",
    "2. are consistent even when the author is writing on different subjects.\n",
    "\n",
    "### Features may works\n",
    "- Lexical features:\n",
    " - The average number of words per sentence\n",
    " - Sentence length variation\n",
    " - Lexical diversity, which is a measure of the richness of the author’s vocabulary\n",
    "- Punctuation features:\n",
    " - Average number of commas, semicolons and colons per sentence\n",
    "- average length of words\n",
    "- the frequency of digits used\n",
    "- the frequency of letters used\n",
    "\n",
    "## References\n",
    "Authorship Attribution with Python\n",
    "http://www.aicbt.com/authorship-attribution/\n",
    "\n",
    "Ultimate guide to deal with Text Data\n",
    "https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "\n",
    "Comprehensive Hands on Guide to Twitter Sentiment Analysis with dataset and code\n",
    "https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/\n",
    "\n",
    "## Supervised Learning\n",
    "(need labels, gather ground truth from external source)\n",
    "k-Nearest Neighbors\n",
    "Linear Regression\n",
    "Logistic Regression\n",
    "Support Vector Machines（SVMs）\n",
    "Decision Trees and Random Forests\n",
    "Neural networks\n",
    "\n",
    "## Unsupervised Learning\n",
    "(do not need labels, the analysis is conducted without ground truth. )\n",
    "- Clustering\n",
    " - k-Means\n",
    " - Hierarchical Cluster Analysis（HCA）\n",
    " - Expectation Maximization\n",
    " \n",
    "- Visualization and dimensionality reduction\n",
    " - Principal Component Analysis（PCA）\n",
    " - Kernel PCA\n",
    " - Locally-Linear Embedding（LLE）\n",
    " - t-distributed Stochastic Neighbor Embedding（t-SNE）\n",
    "- Association rule learning\n",
    " - Apriori\n",
    " - Ecla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal trials\n",
    "consider the unsupervised problem. There are three steps:\n",
    "\n",
    "1. Preparing and loading the data\n",
    "2. Feature extraction: We will experiment with a few different feature sets. Even though the focus is on the unsupervised problem, the feature extraction code can also be used for supervised learning.\n",
    "3. Classification: We will use clustering to find natural groupings in the data. Since we have several feature sets, we will use ensemble learning: learn multiple models, each built using different features, that vote to determine who wrote each chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries and create global data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn\n",
    "import nltk\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer # removal of suffices, like “ing”, “ly”, “s”, etc.\n",
    "from textblob import TextBlob\n",
    "\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8746</td>\n",
       "      <td>@handle Let's try and catch up live next week!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8746</td>\n",
       "      <td>Going to watch Grey's on the big screen - Thur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8746</td>\n",
       "      <td>@handle My pleasure Patrick....hope you are well!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8746</td>\n",
       "      <td>@handle Hi there! Been traveling a lot and lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8746</td>\n",
       "      <td>RT @handle Looking to Drink Clean &amp; Go Green? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              tweet\n",
       "0  8746     @handle Let's try and catch up live next week!\n",
       "1  8746  Going to watch Grey's on the big screen - Thur...\n",
       "2  8746  @handle My pleasure Patrick....hope you are well!\n",
       "3  8746  @handle Hi there! Been traveling a lot and lot...\n",
       "4  8746  RT @handle Looking to Drink Clean & Go Green? ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame\n",
    "df = pd.read_csv('data/train_tweets.txt', header=None, sep='\\t')\n",
    "df.columns = ['id', 'tweet']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of words\n",
    "2. Number of characters(with spaces or without space is giving)\n",
    "3. Average Word Length\n",
    "4. Number of stopwords\n",
    "5. Number of special characters\n",
    "6. Number of numerics\n",
    "7. Number of Uppercase words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df['tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "chars = df['tweet'].str.len() ## this also includes spaces\n",
    "# charNum = df['tweet'].apply(lambda x: \n",
    "#                                    len(str(x).replace(\" \", \"\")))\n",
    "avg_word = df['tweet'].apply(lambda x: avg_word(x))\n",
    "stopwords = df['tweet'].apply(lambda x: \n",
    "                                    len([x for x in x.split() \n",
    "                                         if x in stop]))\n",
    "hastags = df['tweet'].apply(lambda x: \n",
    "                               len([x for x in x.split() \n",
    "                                    if x.startswith('#')]))\n",
    "numerics = df['tweet'].apply(lambda x: \n",
    "                             len([x for x in x.split() \n",
    "                                  if x.isdigit()]))\n",
    "upper = df['tweet'].apply(lambda x: \n",
    "                          len([x for x in x.split() \n",
    "                               if x.isupper()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pre-processing\n",
    "cleaning the data in order to obtain better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['going', 'watch', 'greys', 'big', 'screen', 'thursday', 'indulgence'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preProcess = copy.deepcopy(f['tweet'])\n",
    "# Lower case\n",
    "preProcess = preProcess.apply(lambda x: \n",
    "                                      \" \".join(x.lower() \n",
    "                                               for x in x.split()))\n",
    "# Removing Punctuation\n",
    "preProcess = preProcess.str.replace('[^\\w\\s]','')\n",
    "# Removal of Stop Words\n",
    "preProcess = preProcess.apply(lambda x: \n",
    "                                \" \".join(x for x in x.split() \n",
    "                                         if x not in stop))\n",
    "# Common word removal\n",
    "freq = pd.Series(' '.join(preProcess).split()).value_counts()[:10]\n",
    "freq_index = list(freq.index)\n",
    "preProcess = preProcess.apply(lambda x: \n",
    "                                      \" \".join(x for x in x.split() \n",
    "                                               if x not in freq))\n",
    "# Rare words removal\n",
    "freq = pd.Series(' '.join(preProcess).split()).value_counts()[-10:]\n",
    "freq_index = list(freq.index)\n",
    "preProcess = preProcess.apply(lambda x: \n",
    "                                \" \".join(x for x in x.split() \n",
    "                                         if x not in freq))\n",
    "# Spelling correction(take a lot of time)\n",
    "# preProcess.apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "# Tokenization\n",
    "# TextBlob(preProcess[1]).words\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计词频\n",
    "nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "matchObj = re.match(r'\\S'*)\n",
    "s1 = \"asad a a sdas da as das \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
