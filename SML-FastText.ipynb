{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText - Project 1 - COMP90051 Statistical Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group: Kelloggs\n",
    "Team Members: Dean Pakravan, David Watson, Aaron Qiu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print('Import libraries successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO BE RUN ONLY ONCE - If it is the first time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three cells take in the training data set provided on kaggle. Each tweet author id is attached with a prefix '__label__' . This is for fasttext to recognize the author id's as labels. FastText can alter what the prefix should be if the user desires. You will need to uncomment the following three cells if it is the first time running on your personal machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def readLabelData(path):\n",
    "#     fo = open(path, \"r\", encoding=\"utf8\")\n",
    "#     data = fo.readlines();\n",
    "#     fo.close()\n",
    "#     return data\n",
    "\n",
    "# data = readLabelData('train_tweets.txt')\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to attach __label__ to the author\n",
    "\n",
    "# for i in range(len(data)):\n",
    "#     data[i] = \"__label__\" + data[i]\n",
    "\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def writeLabel(data):\n",
    "#     f= open(\"train_tweet_label.txt\",\"w+\", encoding=\"utf8\")\n",
    "#     for i in range(len(data)):\n",
    "#         f.write(data[i]) \n",
    "#     return\n",
    "\n",
    "# writeLabel(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The remaining code can be run as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our already labeled data\n",
    "def readLabelData(path):\n",
    "    fo = open(path, \"r\", encoding=\"utf8\")\n",
    "    data = fo.readlines();\n",
    "    fo.close()\n",
    "    return data\n",
    "\n",
    "data = readLabelData('train_tweet_label.txt')\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove RE-TWEET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells both remove retweets successfully. The first cell was the first created and performs extremely slow compared to the second one. Kept for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the first letters are RT\n",
    "\n",
    "# def remRetweet(data):\n",
    "#     arr = []\n",
    "#     for i in range(len(data)):\n",
    "#         word = (data[i].split('\\t'))\n",
    "#         # Ignore empty tweets\n",
    "#         if (len(word) >= 2):\n",
    "#             word = word[1]\n",
    "#             # Ignore tweets with less than 2 charecters\n",
    "#             if (len(word) >= 2):\n",
    "#                 word = word[:2]\n",
    "#                 if (word == 'RT'):\n",
    "#                     arr.append(i)\n",
    "#     print(len(arr))            \n",
    "#     for j in range(len(arr)):\n",
    "#         index = arr[j] - j\n",
    "#         del data[index]\n",
    "#     return data\n",
    "\n",
    "# print(len(data))\n",
    "# data = remRetweet(data)\n",
    "# print(len(data))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "# Remove tweets, assumes RT is at the start of the tweet\n",
    "data = [i for i in data if not ('\\tRT' in i )]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check after removing retweet\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-proceses the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if the first time\n",
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "import emoji\n",
    "\n",
    "for i in range(len(data)):\n",
    "    # Remove @ and # (+|(#[A-Za-z0-9]+))\n",
    "    data[i] = ' '.join(re.sub(\"(@[A-Za-z0-9]+|(#[A-Za-z0-9]+))\", \" \", data[i]).split())\n",
    "    # Remove punctuation\n",
    "    data[i] = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", data[i]).split())\n",
    "    # Lowercase\n",
    "    data[i] = data[i].lower()\n",
    "    # Fix misspell words\n",
    "    data[i] = ''.join(''.join(s)[:2] for _, s in itertools.groupby(data[i]))   \n",
    "    #Part for emojis\n",
    "#     data[i] = emoji.demojize(data[i])\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save after pre-processing to avoid repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following rows are commented out as the pre-processing stage constantly changed throughout testing. It is written purely for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def writePred(data):\n",
    "#     f= open(\"train_tweet_label_pred.txt\",\"w+\", encoding=\"utf-8\")\n",
    "#     for i in range(len(data)):\n",
    "#         f.write(data[i] + \"\\n\") \n",
    "#     return\n",
    "\n",
    "# writePred(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pre-processed data\n",
    "# def openPred(path):\n",
    "#     fo = open(path, \"r\", endocidng = \"utf-8\")\n",
    "#     data = fo.readlines();\n",
    "#     fo.close()\n",
    "#     return data\n",
    "\n",
    "# data = openPred(\"train_tweet_label_pred.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample it and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the data\n",
    "# Currently set at 10,000 rows, altered for testing.\n",
    "dataSamp = data[:10000]\n",
    "print(len(dataSamp))\n",
    "\n",
    "def writeSamp(dataSamp):\n",
    "    f= open(\"train_tweet_label_samp.txt\",\"w+\", encoding=\"utf-8\")\n",
    "    for i in range(len(dataSamp)):\n",
    "        f.write(dataSamp[i] + \"\\n\") \n",
    "    return\n",
    "\n",
    "writeSamp(dataSamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO SPLIT THE DATA SET AND WRITE TO A FILE\n",
    "def shuffle_split(infilename, outfilename1, outfilename2):\n",
    "    from random import shuffle\n",
    "\n",
    "    with open(infilename, 'r', encoding = \"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    shuffle(lines)\n",
    "    # append a newline in case the last line didn't end with one\n",
    "    lines[-1] = lines[-1].rstrip('\\n') + '\\n'\n",
    "    traingdata = len(lines)* 90 // 100\n",
    "    with open(outfilename1, 'w',encoding = \"utf8\") as f:\n",
    "        f.writelines(lines[:traingdata])\n",
    "    with open(outfilename2, 'w',encoding = \"utf8\") as f:\n",
    "        f.writelines(lines[traingdata + 1:])\n",
    "\n",
    "shuffle_split('train_tweet_label_samp.txt', 'train_tweet_BIG.txt','train_tweet_valid.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "# Vary the hyper-parameters\n",
    "hyper_params = {\"lr\": 1,\n",
    "    \"epoch\": 5,\n",
    "    \"wordNgrams\": 2, # this is the best\n",
    "    \"dim\": 5,\n",
    "    \"loss\": 'softmax'}     \n",
    "        \n",
    "# Train the model.\n",
    "model = fasttext.train_supervised('train_tweet_BIG.txt', **hyper_params)\n",
    "print(\"Model trained with the hyperparameter \\n {}\".format(hyper_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK PERFORMANCE      \n",
    "result = model.test('train_tweet_BIG.txt')\n",
    "validation = model.test('train_tweet_valid.txt')\n",
    "        \n",
    "# DISPLAY ACCURACY OF TRAINED MODEL\n",
    "text_line = str(hyper_params) + \",accuracy:\" + str(result[1])  + \",validation:\" + str(validation[1]) + '\\n' \n",
    "print(text_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wish to save the model\n",
    "model.save_model(\"model_filename.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load any previous model - only good if we want to retest some test data\n",
    "model = fasttext.load_model(\"model_filename.ftz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To test on the unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our test data\n",
    "def readLabelData(path):\n",
    "    fo = open(path, \"r\", encoding=\"utf8\")\n",
    "    data = fo.readlines();\n",
    "    fo.close()\n",
    "    return data\n",
    "\n",
    "unlabeledData = readLabelData('test_tweets_unlabeled.txt')\n",
    "print(unlabeledData[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \\n\n",
    "for i in range(len(unlabeledData)):\n",
    "    unlabeledData[i] = ' '.join(re.sub(\"[\\n]\", \" \", unlabeledData[i]).split())\n",
    "unlabeledData[:2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre process test data\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "def pre_process_test(data):\n",
    "    for i in range(len(data)):\n",
    "        # Remove @ and #\n",
    "        data[i] = ' '.join(re.sub(\"(@[A-Za-z0-9]+|(#[A-Za-z0-9]+))\", \" \", data[i]).split())\n",
    "        # Remove punctuation\n",
    "        data[i] = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", data[i]).split())\n",
    "        # Lowercase\n",
    "        data[i] = data[i].lower()\n",
    "        # Fix misspell words\n",
    "        data[i] = ''.join(''.join(s)[:2] for _, s in itertools.groupby(data[i]))\n",
    "    return data\n",
    " \n",
    "unlabeledData = pre_process_test(unlabeledData)\n",
    "\n",
    "print(unlabeledData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to predict on the unlabeled data\n",
    "final = model.predict(unlabeledData,k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the submission text file\n",
    "with open(\"submission.txt\", \"w\") as f:\n",
    "    f.write('Id,Predicted\\n')\n",
    "    index = 0\n",
    "    for i in range(len(final[0])):\n",
    "        index += 1\n",
    "        text = ' '.join(re.sub(\"__label__\", \" \", final[0][i][0]).split())\n",
    "        f.write(str(index) + ',' + text + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
